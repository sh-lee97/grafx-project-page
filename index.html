<!DOCTYPE html>
<html lang="en">
<head>
  <style>
    div {
      max-width: 1000px;
      min-width: 600px;
    }
  </style>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Blind Estimation of Audio Processing Graph</title>
</head>
<body>
  <center>
  <h1> Blind Estimation of Audio Processing Graph  </h1>
  Sungho Lee<sup>1</sup>, Jaehyun Park<sup>1</sup>, Seungryeol Paik<sup>1</sup>, and Kyogu Lee<sup>1,2,3</sup> <br>
  <sup>1</sup>Department of Intelligence and Information,
  <sup>2</sup>Interdisciplinary Program in Artificial Intelligence,
  <sup>3</sup>Artificial Intelligence Institute, Seoul National University. <br>
    <br>
    <em>Submitted to ICASSP 2023 (under review).</em>
    <br>
    <br>
    <table style="max-width:960px;">
    <tr>
    <td>
    <hr>
    <b>Abstract</b>
    
    Musicians and audio engineers sculpt and transform their sounds by connecting multiple processors, forming an <em>audio processing graph</em>. 
However, most deep-learning methods overlook this real-world practice and assume fixed graph settings.
To bridge this gap, we develop a system that reconstructs the entire graph from a given reference audio. 
We first generate a realistic graph-reference pair dataset and train a simple blind estimation system composed of a convolutional reference encoder and a transformer-based graph decoder.
We apply our framework to singing voice effects and drum mixing estimation tasks; evaluation results show that our method can reconstruct complex signal routings, including multi-band processing and sidechaining.
<br>
<hr>
  <h3> Audio Samples </h3>
  <b> Singing Voice Effect Estimation </b><br>
  on <em>seen</em> speakers: 
  <a href="sample-pages/singing-seen-1.html">1-10</a>
  <a href="sample-pages/singing-seen-2.html">11-20</a>
  <a href="sample-pages/singing-seen-3.html">21-30</a>
  <a href="sample-pages/singing-seen-4.html">31-40</a>
  <a href="sample-pages/singing-seen-5.html">41-50</a>
  <br>
  on <em>unseen</em> speakers: 
  <a href="sample-pages/singing-unseen-1.html">1-10</a>
  <a href="sample-pages/singing-unseen-2.html">11-20</a>
  <a href="sample-pages/singing-unseen-3.html">21-30</a>
  <a href="sample-pages/singing-unseen-4.html">31-40</a>
  <a href="sample-pages/singing-unseen-5.html">41-50</a>
  <br>
  <br>
  <b> Drum Mixing Estimation </b><br>
  on <em>seen</em> kits: 
  <a href="sample-pages/drum-seen-1.html">1-10</a>
  <a href="sample-pages/drum-seen-2.html">11-20</a>
  <a href="sample-pages/drum-seen-3.html">21-30</a>
  <a href="sample-pages/drum-seen-4.html">31-40</a>
  <a href="sample-pages/drum-seen-5.html">41-50</a>
  <br>
  on <em>unseen</em> kits: 
  <a href="sample-pages/drum-unseen-1.html">1-10</a>
  <a href="sample-pages/drum-unseen-2.html">11-20</a>
  <a href="sample-pages/drum-unseen-3.html">21-30</a>
  <a href="sample-pages/drum-unseen-4.html">31-40</a>
  <a href="sample-pages/drum-unseen-5.html">41-50</a>
  <br>
<hr>
  <h3> Figures </h3>
<center>
    <table style="max-width:600px; border:0px;">
    <tr>
    <td style="max-width:600px; border:0px;">
  <small>
<center>
<img src="blind-estimation.png" alt="blind-estimation" style="width:520px; max-width:100%">
</center>
<p class="small">
The proposed blind estimation system. We first encode the stereo reference audio with a reference encoder. Then, we pass the latent vector into the prototype decoder, which autoregressively decodes the target graph's categorical variables (node types and edge types). Then, we pass the decoded prototype graph into a parameter estimator, which fills the remaining parameter attributes of nodes (processors) and edges (connections). The evaluation results show that this two-stage decoding (prototype decoding followed by the parameter estimation) performs better than the simple single-stage approach.
</p>
<br>
<center>
<img src="tokengt-framework.png" alt="tokengt-framework" style="width:560px; max-width:100%"><br>
</center>
<p class="small">
The proposed prototype decoder and parameter estimator. These two modules are realized as a single transformer encoder model, which takes node tokens and edge tokens as input (the tokenized graph transformer approach). We concatenate i) the latent token from the reference encoder, ii) the task token to differentiate the two tasks, iii) an optional start-of-graph token for the autoregressive graph decoding, and iv) the prototype graph tokens. For the prototype decoding, we add a causal attention mask.
</p>
</center>
  </small>
  </td>
  </tr>
  </table>
</center>
<hr>
</center>
</body>
</html>
